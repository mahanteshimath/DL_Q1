# -*- coding: utf-8 -*-
"""Q1_Single_File_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QLPKdhUN_lFVeGqIosD8N4HEWuWh8c33
"""

#!pip install --quiet torch torchvision torchaudio scikit-learn matplotlib

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import numpy as np
from sklearn.manifold import TSNE
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# --- Hyperparameters & Device ---
batch_size   = 128       # images per batch
lr           = 1e-3      # learning rate
epochs       = 3        # number of epochs
z_dim        = 64        # latent dimension
lambda_W     = 1e-4      # weight decay
# Sparse AE params
beta         = 1e-3      # sparsity weight
rho          = 0.05      # target activation
# Contractive AE params
gamma        = 1e-2      # contractive weight

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# --- Data Loading ---
transform = transforms.ToTensor()  # MNIST in [0,1]
train_ds = datasets.MNIST('.', train=True,  download=True, transform=transform)
test_ds  = datasets.MNIST('.', train=False, download=True, transform=transform)
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)

# --- Define U-Net Auto-Encoder ---
class UNetAE(nn.Module):
    def __init__(self, z_dim=64):
        super().__init__()
        # Encoder: 28x28 -> 7x7 -> z_dim
        self.enc = nn.Sequential(
            nn.Conv2d(1,32,3,2,1), nn.ReLU(),
            nn.Conv2d(32,64,3,2,1), nn.ReLU(),
            nn.Flatten(), nn.Linear(7*7*64, z_dim)
        )
        # Decoder: z_dim -> 7x7 -> 28x28
        self.dec = nn.Sequential(
            nn.Linear(z_dim,7*7*64), nn.ReLU(),
            nn.Unflatten(1,(64,7,7)),
            nn.ConvTranspose2d(64,32,3,2,1,output_padding=1), nn.ReLU(),
            nn.ConvTranspose2d(32,1,3,2,1,output_padding=1), nn.Sigmoid()
        )
    def forward(self, x):
        h = self.enc(x)
        x_hat = self.dec(h)
        return h, x_hat

# Instantiate models
sparse_model     = UNetAE(z_dim).to(device)
contractive_model = UNetAE(z_dim).to(device)

# --- Loss Helpers & Optimizers ---
mse_loss = nn.MSELoss()
opt_sparse = optim.Adam(sparse_model.parameters(), lr=lr)
opt_contra = optim.Adam(contractive_model.parameters(), lr=lr)

def kl_sparsity(rho, rho_hat):
    term1 = rho * torch.log(rho/(rho_hat+1e-10))
    term2 = (1-rho) * torch.log((1-rho)/(1-rho_hat+1e-10))
    return (term1+term2).sum()

def compute_psnr(a,b):
    mse = torch.mean((a-b)**2)
    return float('inf') if mse==0 else 20*torch.log10(1.0/torch.sqrt(mse))

# --- Training Loop ---
for epoch in range(1, epochs+1):
    sparse_model.train()
    contractive_model.train()
    total_s, total_c = 0, 0

    for x, _ in train_loader:
        x = x.to(device)
        x = x.clone().detach().requires_grad_(True)   # for contractive penalty

        # --- Sparse AE training ---
        h_s, xhat_s = sparse_model(x)
        rec_s = mse_loss(xhat_s, x)
        l2_s = sum((p**2).sum() for p in sparse_model.parameters())
        eps     = 1e-6
        rho_hat = h_s.mean(dim=0)                          # empirical mean
        rho_hat = torch.clamp(rho_hat, eps, 1.0 - eps)     # force into (eps,1-eps)
        kl_pen  = kl_sparsity(rho, rho_hat)
        loss_s  = rec_s + lambda_W*l2_s + beta*kl_pen
        opt_sparse.zero_grad()
        loss_s.backward()
        opt_sparse.step()
        total_s += loss_s.item() * x.size(0)

        # --- Contractive AE training ---
        h_c, xhat_c = contractive_model(x)
        rec_c = mse_loss(xhat_c, x)
        l2_c = sum((p**2).sum() for p in contractive_model.parameters())
        grads = torch.autograd.grad(
            outputs=h_c,
            inputs=x,
            grad_outputs=torch.ones_like(h_c),
            create_graph=True,
            retain_graph=True
        )[0]
        contra_pen = grads.pow(2).sum()
        loss_c = rec_c + lambda_W*l2_c + gamma*contra_pen
        opt_contra.zero_grad()
        loss_c.backward()
        opt_contra.step()
        total_c += loss_c.item() * x.size(0)

    print(f"Epoch {epoch:02d} | Sparse: {total_s/len(train_ds):.4f} | Contra: {total_c/len(train_ds):.4f}")

# --- Extract Embeddings & t-SNE ---
embeddings, labels = {}, None
for name, model in [('Sparse', sparse_model), ('Contractive', contractive_model)]:
    model.eval()
    H_list, Y_list = [], []
    with torch.no_grad():
        for x,y in test_loader:
            x = x.to(device)
            h,_ = model(x)
            H_list.append(h.cpu().numpy()); Y_list.append(y.numpy())
    embeddings[name] = np.vstack(H_list)
    labels = np.hstack(Y_list)
    Z = TSNE(n_components=2, init='pca', random_state=42).fit_transform(embeddings[name])
    plt.figure(figsize=(6,5))
    for d in range(10): plt.scatter(Z[labels==d,0], Z[labels==d,1], s=5)
    plt.title(f"t-SNE of {name} AE"); plt.show()

# --- Interpolation Experiments---

# 1. Prepare 20 random pairs from different digit classes
pairs = []
rng = np.random.default_rng(42)
while len(pairs) < 20:
    i, j = rng.integers(0, len(test_ds), 2)
    if test_ds.targets[i] != test_ds.targets[j]:
        pairs.append((i, j))

# 2. Define interpolation weights
alphas = [0, 0.2, 0.4, 0.6, 0.8, 1.0]

# 3. Raw images tensor for direct indexing (shape: 10000×1×28×28)
imgs = test_ds.data.unsqueeze(1).float() / 255.0

# 4. Run experiments for both AEs
for name, model in [('Sparse', sparse_model), ('Contractive', contractive_model)]:
    model.eval()
    psnrs, dh2s = [], []

    # Disable gradient tracking for speed
    with torch.no_grad():
        for i, j in pairs:
            # Grab two images and move to device
            x1 = imgs[i:i+1].to(device)
            x2 = imgs[j:j+1].to(device)

            # Compute their embeddings
            h1, _ = model(x1)
            h2, _ = model(x2)

            for a in alphas:
                # — Input‐space interpolation —
                xa = a * x1 + (1 - a) * x2
                ha, _ = model(xa)
                xha = model.dec(ha)

                # — Latent‐space interpolation —
                h_lin = a * h1 + (1 - a) * h2
                xhl = model.dec(h_lin)

                # — PSNR (handle both tensor and float return types) —
                psnr_t = compute_psnr(xha, xhl)
                if isinstance(psnr_t, torch.Tensor):
                    psnr_val = psnr_t.detach().cpu().item()
                else:
                    psnr_val = psnr_t

                # — Latent deviation ∥h_a – h_lin∥² —
                dh2_val = ((ha - h_lin) ** 2).sum().detach().cpu().item()

                psnrs.append(psnr_val)
                dh2s.append(dh2_val)

    # 5. Report average metrics
    print(f"{name} AE | PSNR avg: {np.mean(psnrs):.2f} dB | <Δh²> avg: {np.mean(dh2s):.4f}")

# --- Classification Accuracy---
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

for name, H in embeddings.items():
    # Build a small pipeline: standard-scaler + logistic regression
    clf = make_pipeline(
        StandardScaler(),
        LogisticRegression(
            solver='lbfgs',                   # default multinomial solver
            max_iter=1000,
            tol=1e-4,
            verbose=0
        )
    )
    clf.fit(H, labels)
    acc = clf.score(H, labels)
    print(f"{name} AE classification accuracy: {acc*100:.2f}%")

import os
from torchvision.utils import save_image

# Define the base directory to save the images
base_dir = '.'

# Define the paths for train and test image directories
train_img_dir = os.path.join(base_dir, 'mnist_train_images')
test_img_dir = os.path.join(base_dir, 'mnist_test_images')

# Create the directories if they don't exist
os.makedirs(train_img_dir, exist_ok=True)
os.makedirs(test_img_dir, exist_ok=True)

# Save the training dataset as images
print("Saving training dataset as images...")
for i, (image, label) in enumerate(train_ds):
    # Create a subdirectory for each label
    label_dir = os.path.join(train_img_dir, str(label))
    os.makedirs(label_dir, exist_ok=True)
    # Save the image
    save_image(image, os.path.join(label_dir, f'Train_{i}.png'))

print("Training dataset saved as images.")

# Save the testing dataset as images
print("Saving testing dataset as images...")
for i, (image, label) in enumerate(test_ds):
    # Create a subdirectory for each label
    label_dir = os.path.join(test_img_dir, str(label))
    os.makedirs(label_dir, exist_ok=True)
    # Save the image
    save_image(image, os.path.join(label_dir, f'Test_{i}.png'))

print("Testing dataset saved as images.")